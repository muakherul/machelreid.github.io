var store = [{
        "title": "Combining Pretrained High Resource Embeddings And Subword Representations For Low Resource Languages",
        "excerpt":"In our paper “Combining Pretrained High Resource Embeddings And Subword Representations For Low Resource Languages” (Accepted to the ICLR 2020 AfricaNLP Workshop) [paper], we explore methods learning word representations for low resource languages, in particular, Xhosa. In particular we look at different combinations of pretrained vectors in high-resource langauges (such...","categories": [],
        "tags": [],
        "url": "http://localhost:4000/2020/03/02/combining-pretrained-high-resource-embeddings-and-subword-representations-for-low-resource-languages.html",
        "teaser": null
      },{
        "title": "A Simple Guide To Non Autoregressive Machine Translation",
        "excerpt":"Generally, when we think about generation in NLP, we think of it as going from left-to-right (or right-to-left) – i.e. you generate one token at a time. For example, take this diagram of this language model below: [diagram] You can see that the prediction of the word word is conditioned...","categories": [],
        "tags": [],
        "url": "http://localhost:4000/2020/07/19/a-simple-guide-to-non-autoregressive-machine-translation.html",
        "teaser": null
      }]
