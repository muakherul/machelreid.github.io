<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-07-21T13:43:11+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Machel Reid</title><subtitle>Research Intern at the University of Tokyo working on natural language processing research</subtitle><entry><title type="html">Combining Pretrained High Resource Embeddings And Subword Representations For Low Resource Languages</title><link href="http://localhost:4000/2020/03/02/combining-pretrained-high-resource-embeddings-and-subword-representations-for-low-resource-languages.html" rel="alternate" type="text/html" title="Combining Pretrained High Resource Embeddings And Subword Representations For Low Resource Languages" /><published>2020-03-02T00:00:00+09:00</published><updated>2020-03-02T00:00:00+09:00</updated><id>http://localhost:4000/2020/03/02/combining-pretrained-high-resource-embeddings-and-subword-representations-for-low-resource-languages</id><content type="html" xml:base="http://localhost:4000/2020/03/02/combining-pretrained-high-resource-embeddings-and-subword-representations-for-low-resource-languages.html">&lt;p&gt;In our paper &lt;em&gt;“Combining Pretrained High Resource Embeddings And Subword Representations For Low Resource Languages”&lt;/em&gt; (Accepted to the ICLR 2020 AfricaNLP Workshop) 
[&lt;a href=&quot;https://arxiv.org/abs/2003.04419&quot;&gt;paper&lt;/a&gt;], we explore methods learning word representations for low resource languages, in particular, &lt;em&gt;Xhosa&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In particular we look at different combinations of pretrained vectors in high-resource langauges (such as GloVe for English) and subword representations using FASTTEXT.&lt;/p&gt;

\[E_M = \texttt{FASTTEXT}(\mathbb{V}),\ E_V = \texttt{GloVe}(\mathbb{V})\]

&lt;h3 id=&quot;approach&quot;&gt;&lt;strong&gt;APPROACH&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Our approach assumes the existence of a vocabulary in our low-resource language \(\mathbb{V} = \{v_1,\dots,v_T\}\), the corresponding translations of the words in \(\mathbb{V}\) in a high-resource language, referred to as \(\mathbb{D} = \{d_1,\dots,d_T\}\), and a pretrained embedding matrix for the high resource language \(E_{HR}\). \(\mathbb{D}\) can either be comprised by individual words, in the case the word in the low-resource language can be accurately mapped to a single word in the high-resource language (e.g. indoda → man); or by a sequence of words in the case that the word in the low-resource language maps to more than one word in the high-resource language (e.g. bethuna → [listen, everyone]). Concretely, our objective is to use both V and D to produce vector representations for each word in \(\mathbb{V}\).&lt;/p&gt;

&lt;p&gt;To leverage the high-resource language, we embed the atomic elements of \(\mathbb{D}\) in \(E_{HR}\) and map the resulting vectors to the corresponding word in \(\mathbb{V}\). In the case of \(d_i\) being a sequence, we take a similar approach to &lt;a href=&quot;https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.12481&quot;&gt;Lazaridou et al. (2017)&lt;/a&gt; and sum the normalized word vectors for each word in \(d_i\) to produce a word representation for the word \(v_i\). We refer to the resulting embedding matrix as \(E_V\). Additionally, we pretrain another embedding matrix \(E_M\) on a corpus in our low resource language using subword information to capture similarity correlated with the morphological nature of words (&lt;a href=&quot;https://arxiv.org/abs/1607.04606&quot;&gt;Bojanowski et al., 2016&lt;/a&gt;). We experiment with the following 4 methods to initialize the word embeddings for our downstream task (the vocabulary for our downstream task might differ from \(\mathbb{V}\)):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Xh&lt;/em&gt;Pre&lt;/strong&gt; - Initialization with \(E_V\) . Words not present in \(E_V\) are initialized with \(E_M\) .&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Xh&lt;/em&gt;Sub&lt;/strong&gt; - Initialization with \(E_M\) only.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;VecMap&lt;/strong&gt; - We learn cross-lingual word embedding mappings by taking two sets of monolingual word embeddings, \(E_V\) and \(E_M\) , and mapping them to a common space following
Artetxe et al. (2018).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Xh&lt;/em&gt;Meta&lt;/strong&gt; - We compute meta-embeddings for every word \(w_i\) by taking the mean of \(E_V(w_i)\)
and \(E_M(w_i)\), following &lt;a href=&quot;https://arxiv.org/abs/1804.05262&quot;&gt;Coates &amp;amp; Bollegala (2018)&lt;/a&gt;. Words not present in \(E_V\) are associated with an UNK token and its corresponding vector.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;&lt;strong&gt;RESULTS&lt;/strong&gt;&lt;/h3&gt;
&lt;h4 id=&quot;bible&quot;&gt;&lt;strong&gt;Bible&lt;/strong&gt;&lt;/h4&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Embedding Configuration&lt;/th&gt;
      &lt;th&gt;BLEU&lt;/th&gt;
      &lt;th&gt;BEER&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Random Initialization&lt;/td&gt;
      &lt;td&gt;21.79&lt;/td&gt;
      &lt;td&gt;21.84&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;VecMap&lt;/td&gt;
      &lt;td&gt;22.46&lt;/td&gt;
      &lt;td&gt;22.03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;Xh&lt;/em&gt;Sub&lt;/td&gt;
      &lt;td&gt;24.65&lt;/td&gt;
      &lt;td&gt;22.79&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;Xh&lt;/em&gt;Pre&lt;/td&gt;
      &lt;td&gt;27.67&lt;/td&gt;
      &lt;td&gt;22.40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;Xh&lt;/em&gt;Meta&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;29.09&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;23.33&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;xhoxexamples&quot;&gt;&lt;strong&gt;XhOxExamples&lt;/strong&gt;&lt;/h4&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Embedding Configuration&lt;/th&gt;
      &lt;th&gt;BLEU-4&lt;/th&gt;
      &lt;th&gt;BEER&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Random Initialization&lt;/td&gt;
      &lt;td&gt;16.08&lt;/td&gt;
      &lt;td&gt;25.30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;VecMap&lt;/td&gt;
      &lt;td&gt;16.38&lt;/td&gt;
      &lt;td&gt;25.42&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;Xh&lt;/em&gt;Sub&lt;/td&gt;
      &lt;td&gt;17.37&lt;/td&gt;
      &lt;td&gt;26.04&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;Xh&lt;/em&gt;Pre&lt;/td&gt;
      &lt;td&gt;17.06&lt;/td&gt;
      &lt;td&gt;25.70&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;Xh&lt;/em&gt;Meta&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;17.77&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;26.44&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As is evident from the results, the &lt;em&gt;Xh&lt;/em&gt;Meta approach performed the highest in all tasks, signifying the necessity for both embedding spaces for more “meaningful” word embeddings for this task.&lt;/p&gt;

&lt;p&gt;For more details, like dataset statistics, please refer to the &lt;em&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.04419&quot;&gt;paper&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;citation&quot;&gt;&lt;strong&gt;CITATION&lt;/strong&gt;&lt;/h3&gt;
&lt;div class=&quot;language-bibtex highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nc&quot;&gt;@misc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;reid2020combining&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;{Combining Pretrained High-Resource Embeddings and Subword Representations for Low-Resource Languages}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;author&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;{Machel Reid and Edison Marrese-Taylor and Yutaka Matsuo}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;year&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;{2020}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;eprint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;{2003.04419}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;archivePrefix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;{arXiv}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;primaryClass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;{cs.CL}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;For the &lt;em&gt;XhOxExamples&lt;/em&gt; data, feel free to download it here: &lt;a href=&quot;/resources/XhOxExamples.zip&quot;&gt;XhOxExamples.zip&lt;/a&gt;&lt;/p&gt;</content><author><name>Machel Reid</name></author><summary type="html">In our paper “Combining Pretrained High Resource Embeddings And Subword Representations For Low Resource Languages” (Accepted to the ICLR 2020 AfricaNLP Workshop) [paper], we explore methods learning word representations for low resource languages, in particular, Xhosa.</summary></entry></feed>