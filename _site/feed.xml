<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-08-30T11:53:07+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Machel Reid</title><subtitle>Research Intern at the University of Tokyo working on natural language processing research</subtitle><entry><title type="html">A Simple Guide To Non Autoregressive Machine Translation</title><link href="http://localhost:4000/2020/08/19/a-simple-guide-to-non-autoregressive-machine-translation.html" rel="alternate" type="text/html" title="A Simple Guide To Non Autoregressive Machine Translation" /><published>2020-08-19T00:00:00+09:00</published><updated>2020-08-19T00:00:00+09:00</updated><id>http://localhost:4000/2020/08/19/a-simple-guide-to-non-autoregressive-machine-translation</id><content type="html" xml:base="http://localhost:4000/2020/08/19/a-simple-guide-to-non-autoregressive-machine-translation.html">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Generally, when we think about generation in NLP, we think of it as going from left-to-right (or right-to-left) – i.e. you generate one token at a time. For example, take this diagram of this language model below:&lt;/p&gt;

&lt;p&gt;[diagram]&lt;/p&gt;

&lt;p&gt;You can see that the prediction of the word &lt;em&gt;word&lt;/em&gt; is conditioned on the previous prediciton of the word &lt;em&gt;another word&lt;/em&gt;. This form of generation is commonly referred to as &lt;strong&gt;autoregressive generation&lt;/strong&gt; - each output is conditioned on the previous one and a sequence is generated sequentially.&lt;/p&gt;

&lt;p&gt;However, what if you could generate all the tokens in your sequence &lt;em&gt;at-once&lt;/em&gt;. Recently, with the introduction of the Transformer – in which sequence-level operations can be computed in parallel – &lt;strong&gt;non-autoregressive&lt;/strong&gt; translation … .&lt;/p&gt;

&lt;p&gt;In this post, I aim to provide a brief overview of exisiting non-autoregressive translation models and summarize some of the recent advances in this domain.&lt;/p&gt;

&lt;h3 id=&quot;autoregressive-generation&quot;&gt;Autoregressive generation&lt;/h3&gt;
&lt;p&gt;Overview of LMs&lt;/p&gt;

&lt;h2 id=&quot;non-autoregressive&quot;&gt;Non autoregressive&lt;/h2&gt;

&lt;h3 id=&quot;non-autoregressive-translation-jiataos-paper&quot;&gt;Non-autoregressive Translation (Jiatao’s paper)&lt;/h3&gt;

&lt;h3 id=&quot;latent-variable-non-autoregressive-translation---raphaels-paper&quot;&gt;Latent Variable Non-autoregressive Translation - Raphael’s paper&lt;/h3&gt;

&lt;h3 id=&quot;cmlms-and-the-endless-derivatives---marjans-paper&quot;&gt;CMLMs (and the endless derivatives) - Marjan’s paper&lt;/h3&gt;

&lt;h3 id=&quot;levt---jiataos-paper&quot;&gt;LevT - Jiatao’s paper&lt;/h3&gt;

&lt;p&gt;###&lt;/p&gt;</content><author><name>Machel Reid</name></author><summary type="html">Introduction Generally, when we think about generation in NLP, we think of it as going from left-to-right (or right-to-left) – i.e. you generate one token at a time. For example, take this diagram of this language model below:</summary></entry><entry><title type="html">Combining Pretrained High Resource Embeddings And Subword Representations For Low Resource Languages</title><link href="http://localhost:4000/2020/03/02/combining-pretrained-high-resource-embeddings-and-subword-representations-for-low-resource-languages.html" rel="alternate" type="text/html" title="Combining Pretrained High Resource Embeddings And Subword Representations For Low Resource Languages" /><published>2020-03-02T00:00:00+09:00</published><updated>2020-03-02T00:00:00+09:00</updated><id>http://localhost:4000/2020/03/02/combining-pretrained-high-resource-embeddings-and-subword-representations-for-low-resource-languages</id><content type="html" xml:base="http://localhost:4000/2020/03/02/combining-pretrained-high-resource-embeddings-and-subword-representations-for-low-resource-languages.html">&lt;p&gt;In our paper &lt;em&gt;“Combining Pretrained High Resource Embeddings And Subword Representations For Low Resource Languages”&lt;/em&gt; (Accepted to the ICLR 2020 AfricaNLP Workshop) 
[&lt;a href=&quot;https://arxiv.org/abs/2003.04419&quot;&gt;paper&lt;/a&gt;], we explore methods learning word representations for low resource languages, in particular, &lt;em&gt;Xhosa&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In particular we look at different combinations of pretrained vectors in high-resource langauges (such as GloVe for English) and subword representations using FASTTEXT.&lt;/p&gt;

\[E_M = \texttt{FASTTEXT}(\mathbb{V}),\ E_V = \texttt{GloVe}(\mathbb{V})\]

&lt;h3 id=&quot;approach&quot;&gt;&lt;strong&gt;APPROACH&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Our approach assumes the existence of a vocabulary in our low-resource language \(\mathbb{V} = \{v_1,\dots,v_T\}\), the corresponding translations of the words in \(\mathbb{V}\) in a high-resource language, referred to as \(\mathbb{D} = \{d_1,\dots,d_T\}\), and a pretrained embedding matrix for the high resource language \(E_{HR}\). \(\mathbb{D}\) can either be comprised by individual words, in the case the word in the low-resource language can be accurately mapped to a single word in the high-resource language (e.g. indoda → man); or by a sequence of words in the case that the word in the low-resource language maps to more than one word in the high-resource language (e.g. bethuna → [listen, everyone]). Concretely, our objective is to use both V and D to produce vector representations for each word in \(\mathbb{V}\).&lt;/p&gt;

&lt;p&gt;To leverage the high-resource language, we embed the atomic elements of \(\mathbb{D}\) in \(E_{HR}\) and map the resulting vectors to the corresponding word in \(\mathbb{V}\). In the case of \(d_i\) being a sequence, we take a similar approach to &lt;a href=&quot;https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.12481&quot;&gt;Lazaridou et al. (2017)&lt;/a&gt; and sum the normalized word vectors for each word in \(d_i\) to produce a word representation for the word \(v_i\). We refer to the resulting embedding matrix as \(E_V\). Additionally, we pretrain another embedding matrix \(E_M\) on a corpus in our low resource language using subword information to capture similarity correlated with the morphological nature of words (&lt;a href=&quot;https://arxiv.org/abs/1607.04606&quot;&gt;Bojanowski et al., 2016&lt;/a&gt;). We experiment with the following 4 methods to initialize the word embeddings for our downstream task (the vocabulary for our downstream task might differ from \(\mathbb{V}\)):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Xh&lt;/em&gt;Pre&lt;/strong&gt; - Initialization with \(E_V\) . Words not present in \(E_V\) are initialized with \(E_M\) .&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Xh&lt;/em&gt;Sub&lt;/strong&gt; - Initialization with \(E_M\) only.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;VecMap&lt;/strong&gt; - We learn cross-lingual word embedding mappings by taking two sets of monolingual word embeddings, \(E_V\) and \(E_M\) , and mapping them to a common space following
Artetxe et al. (2018).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Xh&lt;/em&gt;Meta&lt;/strong&gt; - We compute meta-embeddings for every word \(w_i\) by taking the mean of \(E_V(w_i)\)
and \(E_M(w_i)\), following &lt;a href=&quot;https://arxiv.org/abs/1804.05262&quot;&gt;Coates &amp;amp; Bollegala (2018)&lt;/a&gt;. Words not present in \(E_V\) are associated with an UNK token and its corresponding vector.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;&lt;strong&gt;RESULTS&lt;/strong&gt;&lt;/h3&gt;
&lt;h4 id=&quot;bible&quot;&gt;&lt;strong&gt;Bible&lt;/strong&gt;&lt;/h4&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Embedding Configuration&lt;/th&gt;
      &lt;th&gt;BLEU&lt;/th&gt;
      &lt;th&gt;BEER&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Random Initialization&lt;/td&gt;
      &lt;td&gt;21.79&lt;/td&gt;
      &lt;td&gt;21.84&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;VecMap&lt;/td&gt;
      &lt;td&gt;22.46&lt;/td&gt;
      &lt;td&gt;22.03&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;Xh&lt;/em&gt;Sub&lt;/td&gt;
      &lt;td&gt;24.65&lt;/td&gt;
      &lt;td&gt;22.79&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;Xh&lt;/em&gt;Pre&lt;/td&gt;
      &lt;td&gt;27.67&lt;/td&gt;
      &lt;td&gt;22.40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;Xh&lt;/em&gt;Meta&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;29.09&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;23.33&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h4 id=&quot;xhoxexamples&quot;&gt;&lt;strong&gt;XhOxExamples&lt;/strong&gt;&lt;/h4&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Embedding Configuration&lt;/th&gt;
      &lt;th&gt;BLEU-4&lt;/th&gt;
      &lt;th&gt;BEER&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Random Initialization&lt;/td&gt;
      &lt;td&gt;16.08&lt;/td&gt;
      &lt;td&gt;25.30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;VecMap&lt;/td&gt;
      &lt;td&gt;16.38&lt;/td&gt;
      &lt;td&gt;25.42&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;Xh&lt;/em&gt;Sub&lt;/td&gt;
      &lt;td&gt;17.37&lt;/td&gt;
      &lt;td&gt;26.04&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;Xh&lt;/em&gt;Pre&lt;/td&gt;
      &lt;td&gt;17.06&lt;/td&gt;
      &lt;td&gt;25.70&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;Xh&lt;/em&gt;Meta&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;17.77&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;26.44&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As is evident from the results, the &lt;em&gt;Xh&lt;/em&gt;Meta approach performed the highest in all tasks, signifying the necessity for both embedding spaces for more “meaningful” word embeddings for this task.&lt;/p&gt;

&lt;p&gt;For more details, like dataset statistics, please refer to the &lt;em&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.04419&quot;&gt;paper&lt;/a&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;citation&quot;&gt;&lt;strong&gt;CITATION&lt;/strong&gt;&lt;/h3&gt;
&lt;pre&gt;&lt;code class=&quot;language-bibtex&quot;&gt;@misc{reid2020combining,
    title={Combining Pretrained High-Resource Embeddings and Subword Representations for Low-Resource Languages},
    author={Machel Reid and Edison Marrese-Taylor and Yutaka Matsuo},
    year={2020},
    eprint={2003.04419},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the &lt;em&gt;XhOxExamples&lt;/em&gt; data, feel free to download it here: &lt;a href=&quot;/resources/XhOxExamples.zip&quot;&gt;XhOxExamples.zip&lt;/a&gt;&lt;/p&gt;</content><author><name>Machel Reid</name></author><summary type="html">In our paper “Combining Pretrained High Resource Embeddings And Subword Representations For Low Resource Languages” (Accepted to the ICLR 2020 AfricaNLP Workshop) [paper], we explore methods learning word representations for low resource languages, in particular, Xhosa.</summary></entry></feed>